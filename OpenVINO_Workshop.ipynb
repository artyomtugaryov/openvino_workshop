{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Register and Set Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the workshop is initializing the OpenVINO™ environment in this Jupyter notebook. \n",
    "The OpenVINO™ 2020.1 package have been installed to `intel/openvino/` already.\n",
    "\n",
    "To initialize the OpenVINO™ environment, run the `intel/openvino/bin/setupvars.sh` script.\n",
    "If the prerequisite steps have been done right, you will see the output: \n",
    "\n",
    "```\n",
    "[setupvars.sh] OpenVINO environment initialized\n",
    "OpenVINO Inference Engine version is: ....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ~/intel/openvino_2021/bin/setupvars.sh\n",
    "\n",
    "from openvino import inference_engine as ie\n",
    "print('OpenVINO Inference Engine version: {}'.format(ie.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "##  1. [Introduction](#s1)\n",
    "\n",
    "##  2. [What is SSD MobileNet V2?](#s2)\n",
    "\n",
    "##  3. [Where Can I Find the Model?](#s3)\n",
    "\n",
    "##  4. [Infer SSD MobileNet V2 on TensorFlow](#s4)\n",
    "\n",
    "##  5. [Infer on Real Data on TensorFlow](#s5)\n",
    "\n",
    "##  6. [OpenVINO™ Overview](#s6)\n",
    "\n",
    "##  7. [Model Optimizer - Entry to OpenVINO™](#s7)\n",
    "\n",
    "##  8. [Inference of SSD MobileNet V2 on OpenVINO™ Inference Engine](#s8)\n",
    "\n",
    "##  9. [Accuracy Checker - OpenVINO&trade; Accuracy Validation Framework](#s9)\n",
    "\n",
    "## 10. [Quantize the Model to Low Precision](#s10)\n",
    "\n",
    "## 11. [Post-Trainig Optimization Toolkit](#s11)\n",
    "\n",
    "## 12. [AccuracyAware Algorithm](#s12)\n",
    "\n",
    "## 13. [VNNI - Deep Learning Boost](#s13)\n",
    "\n",
    "## 14. [Get Even Better Performance](#s14)\n",
    "\n",
    "## 15. [Practice](#s15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction<a id='s1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/training_vs_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download COCO 2017 calidation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/datasets/\n",
    "! curl http://images.cocodataset.org/zips/val2017.zip --output data/datasets/coco.zip\n",
    "! curl http://images.cocodataset.org/annotations/annotations_trainval2017.zip --output data/datasets/coco_annotations.zip\n",
    "! unzip data/datasets/coco.zip -d data/datasets/COCO2017\n",
    "! unzip data/datasets/coco_annotations.zip -d data/datasets/COCO2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly for working with paths: os.path\n",
    "import os\n",
    "\n",
    "# working with arrays\n",
    "import numpy as np \n",
    "\n",
    "# path to data for the workshop\n",
    "WORKSHOP_DATA_PATH = os.path.join('.', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in 6 Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once [OpenVINO™](https://docs.openvinotoolkit.org/) is installed, you can run an inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino import inference_engine as ie\n",
    "\n",
    "# Create an instance of the OpenVINO Inference Engine Core \n",
    "# This is the key module of the OpenVINO Inference Engine\n",
    "ie_core = ie.IECore()\n",
    "\n",
    "# Read a network from the Intermediate Representation (IR)\n",
    "network = ie_core.read_network(os.path.join(WORKSHOP_DATA_PATH, 'model.xml'), \n",
    "                               os.path.join(WORKSHOP_DATA_PATH, 'model.bin'))\n",
    "\n",
    "# Find inputs of the model\n",
    "input_layer = next(iter(network.input_info))\n",
    "\n",
    "# Get input shape of the network\n",
    "input_shape = network.input_info[input_layer].input_data.shape\n",
    "\n",
    "# Load the network that was read from the Intermediate Representation (IR) \n",
    "# to the CPU device \n",
    "network_loaded_on_device = ie_core.load_network(network=network, device_name='CPU')\n",
    "\n",
    "# Start an inference of the loaded network and return output data\n",
    "network_loaded_on_device.infer(inputs={input_layer: np.random.rand(*input_shape)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, go to references of [OpenVINO Inference Engine Python API](https://docs.openvinotoolkit.org/latest/ie_python_api/annotated.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/infer.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: What is SSD MobileNet V2?<a id='s2'></a>\n",
    "\n",
    "![](pictures/mobileNet-SSD-network-architecture.png)\n",
    "\n",
    "The `ssd_mobilenet_v2_coco` model is a [Single-Shot multibox Detection (SSD)](https://arxiv.org/pdf/1801.04381.pdf) network for object detection. The model has been trained from the Common Objects in Context (COCO) image dataset.\n",
    "\n",
    "The model input is a blob that consists of a single image of `1x3x300x300` in the `RGB` order.\n",
    "\n",
    "The model output is a typical vector containing the tracked object data. Note that the `class_id` data is now significant and should be used to determine the classification for any detected object.\n",
    "\n",
    "Model outputs:\n",
    "\n",
    "1. Classifier, name - `detection_classes`, contains predicted bounding boxes classes in range `[1, 91]`. The model was trained on Microsoft\\* COCO dataset version with 90 categories of objects.\n",
    "2. Probability, name - `detection_scores`, contains probability of detected bounding boxes.\n",
    "3. Detection box, name - `detection_boxes`, contains detection boxes coordinates in format `[y_min, x_min, y_max, x_max]`, where (`x_min`, `y_min`)  are coordinates top left corner, (`x_max`, `y_max`) are coordinates of the right bottom corner. Coordinates are rescaled to the input image size.\n",
    "4. Detections number, name - `num_detections`, contains the number of predicted detection boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Where Can I Find the Model?<a id='s3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the OpenVINO™ toolkit, you can easily download models from the [Intel&reg; Open Model Zoo](https://github.com/opencv/open_model_zoo), which contains both public open-sourse models from different frameworks (TensorFlow\\*, Caffe\\*, MxNet\\*, PyTorch\\* and others) and models created at Intel&reg;.\n",
    "\n",
    "To see all available models, run the `downloader.py` script with the `--print_all` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to download an object-detection model called `ssd_mobilenet_v2_coco` using the [Model Downloader](https://github.com/opencv/open_model_zoo/tree/master/tools/downloader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py \\\n",
    "--name ssd_mobilenet_v2_coco \\\n",
    "--output_dir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Model Downloader can load not only publicly famous model, but also various models created at Intel for a range of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Downloader downloaded the model to the following directory: `data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Infer SSD MobileNet V2 in TensorFlow<a id='s4'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.gfile import GFile\n",
    "\n",
    "# Path to the TensorFlow model\n",
    "model = os.path.join('data', 'public', \n",
    "                     'ssd_mobilenet_v2_coco', \n",
    "                     'ssd_mobilenet_v2_coco_2018_03_29', \n",
    "                     'frozen_inference_graph.pb')\n",
    "\n",
    "# SSD MobileNet v2 contains following output nodes\n",
    "output_names = ['detection_classes:0','detection_scores:0', 'detection_boxes:0', 'num_detections:0']\n",
    "\n",
    "# Create a graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Create graph definitions\n",
    "graph_def = tf.GraphDef()\n",
    "\n",
    "# Read model to the graph definitions\n",
    "with open(model, \"rb\") as model_file:\n",
    "    graph_def.ParseFromString(model_file.read())\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    # Import the graph definitions to TensorFlow\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "    # Get tensors for output nodes\n",
    "    output_tensors = [graph.get_tensor_by_name(layer_name) for layer_name in output_names] \n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # Infer the model for random datates\n",
    "        print(session.run(output_tensors, feed_dict = {'image_tensor:0' : np.random.rand(1, 300, 300, 3)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/inference_tf.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Infer on Real Data in TensorFlow<a id='s5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the TensorFlow `ssd_mobilenet_v2_coco` model, we need some utility functions and constant values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Initialize logging\n",
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "\n",
    "# Define how many times we run inference to get better performance\n",
    "NUM_RUNS = 1 \n",
    "# Number of images for one inference\n",
    "BATCH = 1\n",
    "\n",
    "# Contains all data for the workshop\n",
    "WORKSHOP_DATA_PATH = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# Path to a test image\n",
    "IMAGE = os.path.join(WORKSHOP_DATA_PATH, 'images', 'input', 'cats.jpg')\n",
    "\n",
    "# Path to the downloaded TensorFlow image\n",
    "SSD_ASSETS = os.path.join(WORKSHOP_DATA_PATH, 'public', 'ssd_mobilenet_v2_coco')\n",
    "\n",
    "# Path to the downloaded frozen TensorFlow image\n",
    "TF_MODEL = os.path.join(SSD_ASSETS,\n",
    "                        'ssd_mobilenet_v2_coco_2018_03_29', \n",
    "                        'frozen_inference_graph.pb')\n",
    "\n",
    "# Create output directory:\n",
    "OUTPUT_PATH = os.path.join(WORKSHOP_DATA_PATH, 'images', 'output')\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Path to the resulting TensorFlow image\n",
    "TF_RESULT_IMAGE = os.path.join(OUTPUT_PATH, 'tensorflow_output.png')\n",
    "\n",
    "# Path to the Inference Engine FP32 model\n",
    "IE_MODEL_FP32_XML = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_FP32_BIN = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "# Path to the Inference Engine INT8 model optimized with the Default algorithm\n",
    "IE_MODEL_DEFAULT_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_DEFAULT_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'default', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "# Path to the Inference Engine INT8 model optimized  with the AccuracyAware algorithm\n",
    "IE_MODEL_AA_INT8_XML = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.xml')\n",
    "IE_MODEL_AA_INT8_BIN = os.path.join(SSD_ASSETS, 'INT8', 'acuracy_aware', 'optimized', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "# Path to the resulting InferenceEngine image\n",
    "IE_RESULT_IMAGE = os.path.join(OUTPUT_PATH, 'inference_engine_output.png')\n",
    "\n",
    "# Path to the combination of the resulting TensorFlow and Inference Engine images\n",
    "COMBO_RESULT_IMAGE = os.path.join(OUTPUT_PATH, 'combo_output.png')\n",
    "\n",
    "PERFORMANCE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV for image processing\n",
    "import cv2\n",
    "\n",
    "def read_resize_image(path_to_image: str, width: int, height: int):\n",
    "    \"\"\"\n",
    "    Takes an image and resizes it to the given dimensions.\n",
    "    \"\"\"\n",
    "    # Load the image \n",
    "    raw_image = cv2.imread(path_to_image)\n",
    "    # Return the image resized to the (width, height) format\n",
    "    return cv2.resize(raw_image, (width, height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required functions from TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "def tf_inference(graph: tf.Graph, input_data, input_name: str, outputs_names: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns TensorFlow model inference results.\n",
    "    \"\"\"\n",
    "    \n",
    "    log.info(\"Running inference with TensorFlow ...\")\n",
    "  \n",
    "    # Get the input tensor by name\n",
    "    input_tensor =  graph.get_tensor_by_name('{}:0'.format(input_name))\n",
    "    \n",
    "    # Fill input data\n",
    "    feed_dict = {\n",
    "        input_tensor: [input_data, ] \n",
    "    }\n",
    "\n",
    "    # Collect output tensors\n",
    "    output_tensors = []\n",
    "    \n",
    "    for output_name in outputs_names:\n",
    "        tensor = graph.get_tensor_by_name('{}:0'.format(output_name))\n",
    "        output_tensors.append(tensor)\n",
    "    \n",
    "    # Run inference and get performance\n",
    "    log.info(\"Running tf.Session\")\n",
    "    with graph.as_default():\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            inference_start = time.time()\n",
    "            outputs = session.run(output_tensors, feed_dict=feed_dict)\n",
    "            inference_end = time.time()\n",
    "    \n",
    "    # Collect inference results\n",
    "    res = dict(zip(outputs_names, outputs))\n",
    "    \n",
    "    log.info(\"TensorFlow reference collected successfully\")\n",
    "    \n",
    "    return res, inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def tf_main(path_to_pb_model: str, \n",
    "            path_to_original_image: str, \n",
    "            number_inference: int = 1):\n",
    "    \"\"\"\n",
    "    Entrypoint to infer with TensorFlow.\n",
    "    \"\"\"\n",
    "    log.info('COMMON: image preprocessing')\n",
    "    \n",
    "    # Size of the image is 300x300 pixels, 3 channels in the RGB format\n",
    "    width = 300\n",
    "    \n",
    "    image_shape = (300, 300, 3)\n",
    "    \n",
    "    resized_image = read_resize_image(path_to_original_image, width, width)\n",
    "    \n",
    "    reshaped_image = np.reshape(resized_image, image_shape)\n",
    "    \n",
    "    log.info('Current shape: {}'.format(reshaped_image.shape))\n",
    "\n",
    "    log.info('TENSORFLOW SPECIFIC: Loading a model with TensorFlow')\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(path_to_pb_model, \"rb\") as model_file:\n",
    "        graph_def.ParseFromString(model_file.read())\n",
    "\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    log.info(\"TensorFlow graph was created\")\n",
    "    \n",
    "    # We use SSD MobileNet V2 and we know the name of the input \n",
    "    input_layer = 'image_tensor'\n",
    "    \n",
    "    # And we know names of outputs\n",
    "    output_layers = ['num_detections', 'detection_classes', 'detection_scores', 'detection_boxes']\n",
    "    \n",
    "    collected_inference_time = []\n",
    "    \n",
    "    for run in range(number_inference):\n",
    "        raw_results, inference_time = tf_inference(graph, reshaped_image, input_layer, output_layers)\n",
    "        collected_inference_time.append(inference_time)\n",
    "    \n",
    "    tensorflow_average_inference_time = sum(collected_inference_time) / number_inference\n",
    "    \n",
    "    log.info('TENSORFLOW SPECIFIC: Plain inference finished')\n",
    "\n",
    "    return raw_results, tensorflow_average_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image class from display to show an image\n",
    "from IPython.display import Image\n",
    "# Show the image in the notebok\n",
    "Image(filename=IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer the Model on a Real Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "framework = 'TF'\n",
    "device = 'CPU'\n",
    "name = '{f} on {d}'.format(f=framework, d=device)\n",
    "\n",
    "tensorflow_fps_collected = []\n",
    "\n",
    "# Run inference in TensorFlow\n",
    "tensorflow_predictions, tensorflow_average_inference_time = tf_main(TF_MODEL, IMAGE, number_inference=NUM_RUNS)\n",
    "    \n",
    "log.info(f'Inference Time of SSD MobileNet V2 {name} is {tensorflow_average_inference_time} seconds')\n",
    "\n",
    "# Calculate FPS from inference time\n",
    "tensorflow_average_fps = 1 / tensorflow_average_inference_time\n",
    "\n",
    "log.info(f'{name} frames per second (FPS): {tensorflow_average_fps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensorflow_predictions['num_detections']) # get number of detected objects\n",
    "print(tensorflow_predictions['detection_classes'][0])# get predicted classes IDs\n",
    "print(tensorflow_predictions['detection_scores'][0]) # get probabilities for predicted classes\n",
    "print(tensorflow_predictions['detection_boxes'][0]) # get boxes for predicted objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions to process images from TensorFlow and draw images\n",
    "from utils import parse_od_output, draw_image\n",
    "\n",
    "# Import the Image class from display to show an image\n",
    "from IPython.display import Image\n",
    "\n",
    "processd_tensorflow_predictions = parse_od_output(tensorflow_predictions)\n",
    "draw_image(IMAGE, processd_tensorflow_predictions, TF_RESULT_IMAGE)\n",
    "\n",
    "# Show the image in the notebok\n",
    "Image(filename=TF_RESULT_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: [OpenVINO&trade;](https://docs.openvinotoolkit.org/) Overview<a id='s6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/infer.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_toolkit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: [Model Optimizer](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) - Entry to OpenVINO&trade;<a id='s7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/model_optimizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's convert the TensorFlow model to the IR format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \\\n",
    "--output_dir=data/public/ssd_mobilenet_v2_coco/FP32 \\\n",
    "--reverse_input_channels \\\n",
    "--model_name=ssd_mobilenet_v2_coco \\\n",
    "--transformations_config=${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json \\\n",
    "--tensorflow_object_detection_api_pipeline_config=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \\\n",
    "--output=detection_classes,detection_scores,detection_boxes,num_detections \\\n",
    "--input_model=data/public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/openvino_support.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the Intermediate Representation of the SSD MobileNet V2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Inference of SSD MobileNet V2 on [OpenVINO™ Inference Engine](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)<a id='s8'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore, IENetwork\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def ie_inference(path_to_model_xml: str, path_to_model_bin: str, path_to_original_image: str, device='CPU', batch=1):\n",
    "    \"\"\"\n",
    "    Entrypoint to infer with the OpenVINO Inference Engine\n",
    "    \"\"\"\n",
    "\n",
    "    # Now let's create the IECore() entity \n",
    "    log.info(\"Creating Inference Engine Core\")   \n",
    "    ie = IECore()\n",
    "\n",
    "    # First, create a network (Note: you need to provide model in the IR previously converted with Model Optimizer)\n",
    "    log.info(\"Reading IR...\")\n",
    "    net = ie.read_network(model=path_to_model_xml, weights=path_to_model_bin)\n",
    "\n",
    "    # Get input and output blob of the network\n",
    "    input_blob = next(iter(net.input_info))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "\n",
    "    # Reshape the network to the needed batch\n",
    "    _, c, h, w = net.input_info[input_blob].input_data.shape\n",
    "    \n",
    "    net.reshape({input_blob: (batch, c, h, w)})\n",
    "    \n",
    "    # Resize the image \n",
    "    log.info('COMMON: image preprocessing')\n",
    "    image = read_resize_image(path_to_original_image, h, w)\n",
    "    \n",
    "    # Now we load the network to the plugin\n",
    "    log.info(\"Loading IR to the plugin...\")\n",
    "    exec_net = ie.load_network(network=net, device_name=device)\n",
    "\n",
    "    del net\n",
    "\n",
    "    labels_map = None\n",
    "    \n",
    "    # Read and preprocess the input image\n",
    "    image = image[..., ::-1]\n",
    "    in_frame = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    batched_frame = np.array([in_frame for _ in range(batch)])\n",
    "    log.info('Current shape: {}'.format(batched_frame.shape))\n",
    "\n",
    "    # Now we run an inference on the target device\n",
    "    inference_start = time.time()\n",
    "    res = exec_net.infer(inputs={input_blob: batched_frame})\n",
    "    inference_end = time.time()\n",
    "\n",
    "    log.info('INFERENCE ENGINE SPECIFIC: no post-processing')\n",
    "\n",
    "    return res[out_blob], inference_end - inference_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_main(xml:str, bin:str, device:str, postfix: str = ''):\n",
    "    name = '{f} {p} on {d}'.format(f='IE', p=postfix, d=device)\n",
    "\n",
    "    inference_engine_fps_collected = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        # Run an inference on OpenVINO Inference Engine\n",
    "        predictions, inference_time = ie_inference(xml, bin,\n",
    "                                                   IMAGE,\n",
    "                                                   device,\n",
    "                                                   batch=BATCH)\n",
    "        \n",
    "        log.info('Inference Time of SSD MobileNet V2 {}: {} seconds'.format(name, inference_time))\n",
    "        # Calculate FPS from inference time\n",
    "        inference_engine_fps = 1 / inference_time\n",
    "        \n",
    "        inference_engine_fps_collected.append(inference_engine_fps)\n",
    "\n",
    "    # Calculate the average FPS for all inferences\n",
    "    inference_engine_avg_fps = (sum(inference_engine_fps_collected) * BATCH) / (NUM_RUNS)\n",
    "    \n",
    "    PERFORMANCE[name] = inference_engine_avg_fps\n",
    "\n",
    "    log.info('{} frames per second (FPS): {}'.format(name, inference_engine_avg_fps))\n",
    "    \n",
    "    return inference_engine_avg_fps, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "\n",
    "# Run the inference \n",
    "inference_engine_average_fps, inference_engine_predictions = ie_main(IE_MODEL_FP32_XML, \n",
    "                                                                     IE_MODEL_FP32_BIN, \n",
    "                                                                     device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_engine_predictions[0] # get data for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_od_output, draw_image\n",
    "\n",
    "draw_image(IMAGE, inference_engine_predictions, IE_RESULT_IMAGE, color=(255, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image class from display to show an image\n",
    "from IPython.display import Image\n",
    "\n",
    "# Show the image in the notebok\n",
    "Image(filename=IE_RESULT_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from Matplotlib to show barcharts\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "def show_results_interactively(tf_image: str, ie_image: str, combination_image: str, ie_fps:float, tf_fps:float):\n",
    "    \"\"\"\n",
    "    Takes paths to three images and shows them with Matplotlib on one screen.\n",
    "    \"\"\"\n",
    "    _ = plt.figure(figsize=(30, 10))\n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(wspace=0.25, hspace=0.05)\n",
    "\n",
    "    titles = [\n",
    "        '(a) TensorFlow',\n",
    "        '(b) Inference Engine',\n",
    "        '(c) TensorFlow and Inference Engine\\n predictions are identical'\n",
    "    ]\n",
    "\n",
    "    for i, path in enumerate([tf_image, ie_image, combination_image]):\n",
    "        img_resized = cv2.imread(path)\n",
    "        ax_plot = plt.subplot(gs1[i])\n",
    "        ax_plot.axis(\"off\")\n",
    "        addon = ' '\n",
    "        if i == 1:\n",
    "            addon += '{:4.3f}'.format(ie_fps) + '(FPS)'\n",
    "        elif i == 0:\n",
    "            addon += '{:4.3f}'.format(tf_fps) + '(FPS)'\n",
    "\n",
    "        ax_plot.text(0.5, -0.5, titles[i] + addon,\n",
    "                     size=28, ha=\"center\",\n",
    "                     transform=ax_plot.transAxes)\n",
    "        ax_plot.imshow(cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import draw_image\n",
    "\n",
    "# Draw inference results from the Inference Engine in the image with TensorFlow inference results\n",
    "draw_image(TF_RESULT_IMAGE, inference_engine_predictions, COMBO_RESULT_IMAGE, color=(255, 0, 0))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=inference_engine_average_fps,\n",
    "                           tf_fps=tensorflow_average_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_performance\n",
    "\n",
    "performance_data = {\n",
    "    'TF on CPU': tensorflow_average_fps,\n",
    "    'IE on CPU': inference_engine_average_fps\n",
    "}\n",
    "\n",
    "show_performance(performance_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/ie_fusing.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oh, this is good - we got the same results in one image. But it is only ONE image. We need to check accuracy on the whole dataset. So how can we do this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: [Accuracy Checker](https://github.com/opencv/open_model_zoo/tree/master/tools/accuracy_checker) - OpenVINO&trade; Accuracy Validation Framework<a id='s9'></a>\n",
    "\n",
    "![](pictures/accuracy_check.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace variables to the real path in the Accuracy Checker config:\n",
    "!WORKSHOP_PATH=$(pwd) envsubst '\\${WORKSHOP_PATH}' <data/configs/accuracy_checker_config_tf_template.yml >data/configs/accuracy_checker_config_tf.yml\n",
    "\n",
    "# Run the Accuracy Checker:\n",
    "!accuracy_check -c data/configs/accuracy_checker_config_tf.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace variables to the real path in the Accuracy Checker config:\n",
    "!WORKSHOP_PATH=$(pwd) envsubst '\\${WORKSHOP_PATH}' <data/configs/accuracy_checker_config_template.yml >data/configs/accuracy_checker_config.yml\n",
    "\n",
    "# Run the Accuracy Checker:\n",
    "!accuracy_check -c data/configs/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/configs/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10: [Quantize the Model to Low Precision](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_README.html)<a id='s10'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/quantization.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/quantize.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 11: [Post-Trainig Optimization Toolkit](https://docs.openvinotoolkit.org/latest/_README.html)<a id='s11'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Post-Training Optimization Toolkit includes standalone command-line tool and Python* API that provide the following key features:\n",
    "\n",
    "* Two supported post-training quantization algorithms: fast [DefaultQuantization](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_default_README.html) and precise [AccuracyAwareQuantization](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_accuracy_aware_README.html).\n",
    "as well as multiple experimental methods including global optimization.\n",
    "* Symmetric and asymmetric quantization schemes. For more details, see the [Quantization](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_README.html) section.\n",
    "* Compression for different hardware targets such as CPU, GPU.\n",
    "* Per-channel quantization for Convolutional and Fully-Connected layers.\n",
    "* Multiple domains: Computer Vision, Recommendation Systems.\n",
    "* Ability to implement custom calibration pipeline via supported [API](https://docs.openvinotoolkit.org/latest/_sample_README.html).\n",
    "\n",
    "<br>   \n",
    "\n",
    "![](pictures/pot.png)\n",
    "\n",
    "<br>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON files with all info required for calibration:\n",
    "1. Model parameters (name, path to full precision IR)   \n",
    "2. Engine Parameters (e.g. preprocessing parameters, dataset path, etc.)    \n",
    "    2.1 Use accuracy checker .yml config   \n",
    "    2.2 Define all the required AccuracyChecker parameters directly in the JSON file    \n",
    "3. Compression parameters (optimization algorithm and its parameters)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DefaultQuantization algorithm performs a fast but at the same time accurate INT8 calibration of NNs. It consists of three algorithms that are sequentially applied to a model:\n",
    "*  ActivationChannelAlignment - Used as a preliminary step before quantization and allows you to align ranges of output activations of Convolutional layers in order to reduce the quantization error.\n",
    "*  MinMaxQuantization - This is a vanilla quantization method that automatically inserts `FakeQuantize` operations into the model graph based on the specified  target hardware and initializes them\n",
    "using statistics collected on the calibration dataset.\n",
    "*  BiasCorrection - Adjusts biases of Convolutional and Fully-Connected layers based on the quantization error of the layer in order to make the overall error unbiased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py \\\n",
    "-c data/configs/default/quantization_config.json \\\n",
    "--output-dir data/public/ssd_mobilenet_v2_coco/INT8/default \\\n",
    "--direct-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/configs/default/quantization_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/quantized_ir.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [FakeQuantize Layer](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_IR_V10_opset1.html#FakeQuantize) <a name=\"FakeQuantize\"></a>\n",
    "\n",
    "**Name**: *FakeQuantize*\n",
    "\n",
    "**Category**: *Layer*\n",
    "\n",
    "**Short description**: *FakeQuantize* layer is element-wise linear quantization of floating-point input values into a discrete set of floating-point values.\n",
    "\n",
    "**Detailed description**: Input and output ranges as well as the number of levels of quantization are specified by dedicated inputs and attributes. There can be different limits for each element or groups of elements (channels) of the input blobs. Otherwise, one limit applies to all elements. It depends on shape of inputs that specify limits and regular broadcasting rules applied for input blobs. The output of the operator is a floating-point number of the same type as the input blob. In general, there are four values that specify quantization for each element: *input_low*, *input_high*, *output_low*, *output_high*. *input_low* and *input_high* parameters specify the input range of quantization. All input values that are outside this range are clipped to the range before actual quantization. *output_low* and *output_high* specify minimum and maximum quantized values at the output.\n",
    "\n",
    "**Parameters**: *Quantize* layer parameters are specified in the `data` node, which is a child of the `layer` node.\n",
    "\n",
    "* **Parameter name**: *levels*\n",
    "\n",
    "  * **Description**: *levels* is the number of quantization levels.\n",
    "  * **Range of values**: an integer greater than or equal to 2\n",
    "  * **Type**: `int`\n",
    "  * **Default value**: None\n",
    "  * **Required**: *yes*\n",
    "\n",
    "**Inputs**:\n",
    "\n",
    "*   **1**: `X` - multidimensional input blob to quantize. Required.\n",
    "\n",
    "*   **2**: `input_low` - minimum limit for input value. The shape must be broadcastable to the shape of `X`. Required.\n",
    "\n",
    "*   **3**: `input_high` - maximum limit for input value. Can be the same as `input_low` for binarization. The shape must be broadcastable to the shape of `X`. Required.\n",
    "\n",
    "*   **4**: `output_low` - minimum quantized value. The shape must be broadcastable to the shape of `X`. Required.\n",
    "\n",
    "*   **5**: `output_high` - maximum quantized value. The shape must be broadcastable to the of `X`. Required.\n",
    "\n",
    "**Mathematical Formulation**\n",
    "\n",
    "Each element of the output is defined as the result of the following expression:\n",
    "\n",
    "```python\n",
    "if x <= input_low:\n",
    "    output = output_low\n",
    "elif x > input_high:\n",
    "    output = output_high\n",
    "else:\n",
    "    # input_low < x <= input_high\n",
    "    output = round((x - input_low) / (input_high - input_low) * (levels-1)) / (levels-1) * (output_high - output_low) + output_low\n",
    "```\n",
    "\n",
    "**Example**\n",
    "```xml\n",
    "<layer … type=\"FakeQuantize\"…>\n",
    "    <data levels=\"2\"/>\n",
    "    <input>\n",
    "        <port id=\"0\">\n",
    "            <dim>1</dim>\n",
    "            <dim>64</dim>\n",
    "            <dim>56</dim>\n",
    "            <dim>56</dim>\n",
    "        </port>\n",
    "        <port id=\"1\">\n",
    "            <dim>1</dim>\n",
    "            <dim>64</dim>\n",
    "            <dim>1</dim>\n",
    "            <dim>1</dim>\n",
    "        </port>\n",
    "        <port id=\"2\">\n",
    "            <dim>1</dim>\n",
    "            <dim>64</dim>\n",
    "            <dim>1</dim>\n",
    "            <dim>1</dim>\n",
    "        </port>\n",
    "        <port id=\"3\">\n",
    "            <dim>1</dim>\n",
    "            <dim>1</dim>\n",
    "            <dim>1</dim>\n",
    "            <dim>1</dim>\n",
    "        </port>\n",
    "        <port id=\"4\">\n",
    "            <dim>1</dim>\n",
    "            <dim>1</dim>\n",
    "            <dim>1</dim>\n",
    "            <dim>1</dim>\n",
    "        </port>\n",
    "    </input>\n",
    "    <output>\n",
    "        <port id=\"5\">\n",
    "            <dim>1</dim>\n",
    "            <dim>64</dim>\n",
    "            <dim>56</dim>\n",
    "            <dim>56</dim>\n",
    "        </port>\n",
    "    </output>\n",
    "</layer>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_main(IE_MODEL_DEFAULT_INT8_XML, IE_MODEL_DEFAULT_INT8_BIN, device, 'INT8 D')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tensorflow_average_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/configs/default/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace variables to the real path in the Accuracy Checker config:\n",
    "!WORKSHOP_PATH=$(pwd) envsubst '\\${WORKSHOP_PATH}' <data/configs/default/accuracy_checker_config_template.yml >data/configs/default/accuracy_checker_config.yml\n",
    "\n",
    "# Run the Accuracy Checker\n",
    "!accuracy_check -c data/configs/default/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 12: [AccuracyAware Algorithm](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_accuracy_aware_README.html)<a id='s12'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/tools/post_training_optimization_toolkit/main.py \\\n",
    "-c data/configs/accuracy_aware/quantization_config.json \\\n",
    "--output-dir data/public/ssd_mobilenet_v2_coco/INT8/acuracy_aware \\\n",
    "--direct-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat data/configs/accuracy_aware/quantization_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The model gets fully quantized using the DefaultQuantization algorithm.\n",
    "2. The quantized and full-precision models are compared on a subset of the validation set in order to find mismatches in the target accuracy metric. A ranking subset is extracted based on the mismatches.\n",
    "3. A layer-wise ranking is performed in order to get a contribution of each quantized layer into the accuracy drop.\n",
    "4. Based on the ranking, the most \"problematic\" layer is reverted back to the original precision. This change is followed by the evaluation of the obtained model on the full validation set in order to get a new accuracy drop.\n",
    "5. If the accuracy criteria are satisfied for all pre-defined accuracy metrics, the algorithm finishes. Otherwise, it continues reverting the next \"problematic\" layer.\n",
    "6. It may happen that regular reverting does not get any accuracy improvement or even worsen the accuracy. Then the re-ranking is triggered as it is described in step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "ie_avg_fps, predictions = ie_main(IE_MODEL_AA_INT8_XML, IE_MODEL_AA_INT8_BIN, device, 'INT8 AA')\n",
    "\n",
    "draw_image(TF_RESULT_IMAGE, predictions, COMBO_RESULT_IMAGE, color=(0, 0, 255))\n",
    "\n",
    "show_results_interactively(tf_image=TF_RESULT_IMAGE,\n",
    "                           ie_image=IE_RESULT_IMAGE,\n",
    "                           combination_image=COMBO_RESULT_IMAGE,\n",
    "                           ie_fps=ie_avg_fps,\n",
    "                           tf_fps=tensorflow_average_fps)\n",
    "\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace variables to the real path in the Accuracy Checker config:\n",
    "!WORKSHOP_PATH=$(pwd) envsubst '\\${WORKSHOP_PATH}' <data/configs/accuracy_aware/accuracy_checker_config_template.yml >data/configs/accuracy_aware/accuracy_checker_config.yml\n",
    "\n",
    "# Run the Accuracy Checker\n",
    "!accuracy_check -c data/configs/accuracy_aware/accuracy_checker_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 13: VNNI - [Deep Learning Boost](https://www.intel.ai/intel-deep-learning-boost/)<a id='s13'></a>\n",
    "\n",
    "![](pictures/dl_boost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 14: [Get Even Better Performance](https://docs.openvinotoolkit.org/latest/_docs_performance_benchmarks.html)<a id='s14'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great performance results! But if you want the best performance, use C++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a [C++ benchmark application](https://docs.openvinotoolkit.org/latest/_inference_engine_samples_benchmark_app_README.html) in Inference samples. Let's build it from sources and try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${HOME}/inference_engine_cpp_samples_build/intel64/Release/benchmark_app -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${HOME}/inference_engine_cpp_samples_build/intel64/Release/benchmark_app -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!${HOME}/inference_engine_cpp_samples_build/intel64/Release/benchmark_app -m data/public/ssd_mobilenet_v2_coco/FP32/ssd_mobilenet_v2_coco.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFORMANCE['OpenVINO IE Benchmark INT8'] = <PLACEHOLDER>\n",
    "show_performance(PERFORMANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 15: Practice (Part 1)<a id='s15'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use this model to detect objects in a video. Fill in the gaps in the code blocks below. Refer to the previous cells for hints. For more information about the OpenVINO Inference Engine Python API, see the [official documentation](https://docs.openvinotoolkit.org/latest/ie_python_api/annotated.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Contains all data for the workshop\n",
    "WORKSHOP_DATA_PATH = os.path.join('.', 'data')\n",
    "\n",
    "# Path to the downloaded TensorFlow image\n",
    "SSD_ASSETS = os.path.join(WORKSHOP_DATA_PATH, 'public', 'ssd_mobilenet_v2_coco')\n",
    "\n",
    "# Path to the Inference Engine FP32 model\n",
    "# But you can use the INT8 model instead\n",
    "MODEL_PATH_XML = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.xml')\n",
    "MODEL_PATH_BIN = os.path.join(SSD_ASSETS, 'FP32', 'ssd_mobilenet_v2_coco.bin')\n",
    "\n",
    "DEVICE = 'CPU'\n",
    "\n",
    "DATA_PATH = os.path.join('practice', 'data')\n",
    "INPUT_VIDEO = os.path.join(DATA_PATH, 'input.mp4')\n",
    "OUTPUT_VIDEO = os.path.join(DATA_PATH, 'output.MP4')\n",
    "\n",
    "LABELS_PATH = os.path.join(DATA_PATH, 'coco_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "# Show a source video\n",
    "HTML(\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{}\" type=\"video/mp4\"></video>\"\"\".format(INPUT_VIDEO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prapare_out_video_stream(input_video_stream):\n",
    "    width  = int(input_video_stream.get(3))\n",
    "    height = int(input_video_stream.get(4))\n",
    "    return cv2.VideoWriter(OUTPUT_VIDEO, cv2.VideoWriter_fourcc(*'X264'), 20, (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV for work with a video and images\n",
    "import cv2\n",
    "\n",
    "# Import the Inference Engine\n",
    "from openvino.inference_engine import IECore, IENetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create an instance of the OpenVINO Inference Engine `IECore` class\n",
    "This class represents an Inference Engine entity \n",
    "and allows you to manipulate plugins using unified interfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read the prepared model\n",
    "\n",
    "The `IENetwork` class is designed to work with a model in the Inference Engine.\n",
    "This class contains information about the network model read from the Intermediate Representation\n",
    "and allows you to manipulate some model parameters such as layers affinity and output layers.\n",
    "\n",
    "You need to create an instance of the IENetwork class.\n",
    "A constructor of this class has two parameters: \n",
    " 1. path to the .xml file of the model \n",
    " 2. path to the .bin file of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get the name of the input layer of the model\n",
    "\n",
    "To infer a model, you need to know input layers of the model\n",
    "The object `net` contains information about inputs of the network in a property `inputs`,\n",
    "which is a dictionary: key - name of the input layer, volume - representation of the input network.\n",
    "In this case, you need to get only the name of the input. `input_blob` should be a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_blob =\n",
    "\n",
    "print('Input layer of the network is {}'.format(input_blob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get shape (dimensions) of the input layer of the network\n",
    "\n",
    "* n - number of batches\n",
    "* c - number of input image channels (usualy 3 - R, G and B) \n",
    "* h - height\n",
    "* w - width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, c, h, w = \n",
    "\n",
    "print(f'Input shape of the network: [{n}, {c}, {h}, {w}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Get names of output layers of the network\n",
    "\n",
    "For easier processing of inferences result, we should know names of outputs.\n",
    "The object `net` contains information about outputs in the same way as inputs.\n",
    "There is the property `outputs` in the class `IENetwork`.\n",
    "You need to get the name of the output. `out_blob` should be a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_blob = \n",
    "\n",
    "print(f'Output layer of the network: {out_blob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load names of COCO classes from a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load names of COCO classes from a file \n",
    "with open(LABELS_PATH, 'r') as f:\n",
    "    labels_map = [x.strip() for x in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Load the network to a device\n",
    "\n",
    "Use the instance of `IECore`.\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\n",
    "This function prepares the network for the first inference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `network` - instance of `IENetwork`\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_loaded_to_device = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Open the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(INPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create an output video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = prapare_out_video_stream(input_video_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Function for processing inference results\n",
    "\n",
    "The output layer of the SSD MobileNet V2 is the DetectionOutput layer.\n",
    "Data layout of this layer (and `obj` variable):\n",
    " * element 0 - the batch ID (not important in our case)\n",
    " * element 1 - the class ID of a discovered object \n",
    " * element 2 - the confidence for the object\n",
    " * element 3 and 4 - coordinates of the upper-left corner of the box of the object\n",
    " * element 5 and 6 - coordinates of bottom-right corner of the box of the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes_in_frame(obj, frame_w, frame_h, labels_map):\n",
    "    threshold = 0.0001\n",
    "        \n",
    "    # Step 13: Get the confidence for a discovered object\n",
    "    confidence =\n",
    "        \n",
    "    # Step 14: Draw bounding boxes\n",
    "    # Draw a bounding box only for objects the confidence of which is greater than a specified threshold\n",
    "    if confidence > threshold:\n",
    "        # Get coordinates of a discovered object\n",
    "        xmin =\n",
    "        ymin =\n",
    "            \n",
    "        xmax =\n",
    "        ymax =\n",
    "            \n",
    "        # and scale it to the original size of the frame\n",
    "        scaled_xmin = int( xmin * frame_w )\n",
    "        scaled_ymin = int( ymin * frame_h )\n",
    "            \n",
    "        scaled_xmax = int( xmax * frame_w )\n",
    "        scaled_ymax = int( ymax * frame_h )\n",
    "\n",
    "        # Get class ID of a discovered object\n",
    "        class_id =\n",
    "\n",
    "        # Get confidence for a discovered object\n",
    "        confidence = round(confidence * 100, 1)\n",
    "\n",
    "        # Draw a box and a label\n",
    "        color = (min(class_id * 12.5, 255), min(class_id * 7, 255), min(class_id * 5, 255))\n",
    "        cv2.rectangle(frame, (scaled_xmin, scaled_ymin), (scaled_xmax, scaled_ymax), color, 2)\n",
    "\n",
    "        # Get the label of a class\n",
    "        label = labels_map[class_id]\n",
    "\n",
    "        # Create the title of an object\n",
    "        text = '{}: {}% '.format(label, confidence)\n",
    "\n",
    "        # Put the title to a frame\n",
    "        cv2.putText(frame, text, (scaled_xmin, scaled_ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Loop over frames in the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while input_video_stream.isOpened():\n",
    "    \n",
    "    # Read the next frame from the intput video \n",
    "    ret, frame = input_video_stream.read()\n",
    "    # Check if the video is over\n",
    "    if not ret:\n",
    "        # Exit from the loop if the video is over\n",
    "        break \n",
    "    # Get height and width of the frame\n",
    "    frame_h, frame_w = frame.shape[:2]\n",
    "    \n",
    "    # Resize the frame to the network input \n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    \n",
    "    # Change the data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    \n",
    "    # Reshape the frame to the network input \n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    \n",
    "    # To infer the frame, prepare the data.\n",
    "    # This must be a dictionary: \n",
    "    #   key - name of the input layer (you get this early)\n",
    "    #   value - input data (the prepared frame)  \n",
    "    feed_dict = {\n",
    "    }\n",
    "    \n",
    "    # All is ready for the main thing - inference!\n",
    "    # You have read and loaded the network to the device, prepared input data and now you are ready to infer.\n",
    "    \n",
    "    # Step 11:\n",
    "    # To start an inference, call the `infer` function of the `network_loaded_to_device` variable. \n",
    "    # We must set input data (a dictionary).\n",
    "    inference_result = \n",
    "    \n",
    "    # Great! The `inference_result` variable contains output data after inference of the network.\n",
    "    # `inference_result` is a dictionary, \n",
    "    #  where key is the name of the output name, \n",
    "    #        value is data from the blob.\n",
    "    \n",
    "    # Step 12: Then iterate over all discovered objects\n",
    "    for obj in inference_result[out_blob][0][0]:\n",
    "        # You need to run the draw_boxes_in_frame function :\n",
    "        draw_boxes_in_frame()\n",
    "    \n",
    "    # Write the resulting frame to the output stream\n",
    "    out.write(frame)\n",
    "\n",
    "# Save the resulting video\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Show a source video\n",
    "HTML(\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{}\" type=\"video/mp4\"></video>\"\"\".format(OUTPUT_VIDEO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see boxes in the video? \n",
    "If yes, you did all right!\n",
    "**Good Work!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 16: Practice (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the next step? Often from neural networks build pipelines. It is to use the results of the first neural network as an input for the next neural network. \n",
    "Let's try to build a pipeline from two networks:  first is finds a person on the video and the next to recognize the emotions of this person\n",
    "\n",
    "We have already run the first network. And find the person on the video.\n",
    "The next step is to find a network for emotion recognition.\n",
    "There is a good neural network in the [OpenModelZOO](https://docs.openvinotoolkit.org/2019_R1/_docs_Pre_Trained_Models.html) - [emotions-recognition-retail-0003 network](https://docs.openvinotoolkit.org/2019_R1/_emotions_recognition_retail_0003_description_emotions_recognition_retail_0003.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download emotions-recognition-retail-0003 network\n",
    "Run the Model Downloader eith needed arguments to download the emotions-recognition-retail-0003 network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/downloader/downloader.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mode already is in OpenVINO format and you do not need to convert it.\n",
    "\n",
    "After downloading the model you can use it:\n",
    "\n",
    "### Step 2: Read the prepared model\n",
    "The IENetwork class is designed to work with a model in the Inference Engine. This class contains information about the network model read from the Intermediate Representation and allows you to manipulate some model parameters such as layers affinity and output layers.\n",
    "\n",
    "You need to create an instance of the IENetwork class. A constructor of this class has two parameters:\n",
    "\n",
    "path to the .xml file of the model\n",
    "path to the .bin file of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_network = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the network to a device\n",
    "\n",
    "Use the instance of `IECore`.\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\n",
    "This function prepares the network for the first inference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `network` - instance of `IENetwork`\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_network_loaded_on_device ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Open the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(INPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create an output video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = prapare_out_video_stream(input_video_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Prepare a frame and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_infer(frame):\n",
    "    # Find inputs of the model\n",
    "    em_input_layer = \n",
    "\n",
    "     # Get input shape of the network\n",
    "    n, c, h, w = emotion_recognition_network.inputs[em_input_layer].shape\n",
    "\n",
    "    # Resize the frame to the network input \n",
    "    em_in_frame = cv2.resize(frame, (w, h))\n",
    "    \n",
    "    # Change the data layout from HWC to CHW\n",
    "    em_in_frame = em_in_frame.transpose((2, 0, 1))  \n",
    "    \n",
    "    # Reshape the frame to the network input \n",
    "    em_in_frame = em_in_frame.reshape((n, c, h, w))\n",
    "    \n",
    "    # Find inputs of the model\n",
    "    em_output_layer = \n",
    "    \n",
    "    # Run the inference how you did it early\n",
    "    em_results =\n",
    "    \n",
    "    # For understanding what is the result of inference this model, check documentation \n",
    "    # https://docs.openvinotoolkit.org/latest/_models_intel_emotions_recognition_retail_0003_description_emotions_recognition_retail_0003.html\n",
    "    return em_results[em_output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Drow boxes and emotions in a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes_and_emotion_in_frame(obj, frame_w, frame_h, labels_map):\n",
    "    threshold = 0.0001\n",
    "        \n",
    "    # Step 13: Get the confidence for a discovered object\n",
    "    confidence =\n",
    "        \n",
    "    # Step 14: Draw bounding boxes\n",
    "    # Draw a bounding box only for objects the confidence of which is greater than a specified threshold\n",
    "    if confidence > threshold:\n",
    "        # Get coordinates of a discovered object\n",
    "        xmin =\n",
    "        ymin =\n",
    "            \n",
    "        xmax =\n",
    "        ymax =\n",
    "            \n",
    "        # and scale it to the original size of the frame\n",
    "        scaled_xmin = int( xmin * frame_w )\n",
    "        scaled_ymin = int( ymin * frame_h )\n",
    "            \n",
    "        scaled_xmax = int( xmax * frame_w )\n",
    "        scaled_ymax = int( ymax * frame_h )\n",
    "\n",
    "        # Get class ID of a discovered object\n",
    "        class_id =\n",
    "\n",
    "        # If class is person, run inference of emotions-recognition-retail-0003\n",
    "        if class_id == 1:\n",
    "            person_image = frame[scaled_ymin:scaled_ymax, scaled_xmin:scaled_xmax]\n",
    "            # Run emotions-recognition-retail-0003\n",
    "            emotions_prob = \n",
    "            \n",
    "            emotions = ['neutral', 'happy', 'sad', 'surprise', 'angry']\n",
    "            emotion = emotions[np.argmax(emotions_prob)]\n",
    "            \n",
    "        # Get confidence for a discovered object\n",
    "        confidence = round(confidence * 100, 1)\n",
    "\n",
    "        # Draw a box and a label\n",
    "        color = (min(class_id * 12.5, 255), min(class_id * 7, 255), min(class_id * 5, 255))\n",
    "        cv2.rectangle(frame, (scaled_xmin, scaled_ymin), (scaled_xmax, scaled_ymax), color, 2)\n",
    "\n",
    "        # Get the label of a class\n",
    "        label = labels_map[class_id]\n",
    "\n",
    "        # Create the title of an object\n",
    "        text = '{}: {}% '.format(label, confidence)\n",
    "        text = '{} {}'.format(text, emotion) if class_id == 1 else text\n",
    "        # Put the title to a frame\n",
    "        cv2.putText(frame, text, (scaled_xmin, scaled_ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Loop over frames in the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "while input_video_stream.isOpened():\n",
    "    \n",
    "    # Read the next frame from the intput video \n",
    "    ret, frame = input_video_stream.read()\n",
    "    # Check if the video is over\n",
    "    if not ret:\n",
    "        # Exit from the loop if the video is over\n",
    "        break \n",
    "    # Get height and width of the frame\n",
    "    frame_h, frame_w = frame.shape[:2]\n",
    "    \n",
    "    # Resize the frame to the network input \n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    \n",
    "    # Change the data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    \n",
    "    # Reshape the frame to the network input \n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    \n",
    "    # To infer the frame, prepare the data.\n",
    "    # This must be a dictionary: \n",
    "    #   key - name of the input layer (you get this early)\n",
    "    #   value - input data (the prepared frame)  \n",
    "    feed_dict = {\n",
    "    }\n",
    "    \n",
    "    # All is ready for the main thing - inference!\n",
    "    # You have read and loaded the network to the device, prepared input data and now you are ready to infer.\n",
    "    \n",
    "    # Step 11:\n",
    "    # To start an inference, call the `infer` function of the `network_loaded_to_device` variable. \n",
    "    # We must set input data (a dictionary).\n",
    "    inference_result = \n",
    "    \n",
    "    # Great! The `inference_result` variable contains output data after inference of the network.\n",
    "    # `inference_result` is a dictionary, \n",
    "    #  where key is the name of the output name, \n",
    "    #        value is data from the blob.\n",
    "    \n",
    "    # Step 12: Then iterate over all discovered objects\n",
    "    for obj in inference_result[out_blob][0][0]:\n",
    "        draw_boxes_and_emotion_in_frame() # you need to run draw_boxes_and_emotion_in_frame with a needed argument\n",
    "\n",
    "    # Write the resulting frame to the output stream\n",
    "    out.write(frame)\n",
    "\n",
    "# Save the resulting video\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the person (Artyom) on the resulting video will be detected with emotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Show a source video\n",
    "HTML(\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{}\" type=\"video/mp4\"></video>\"\"\".format(OUTPUT_VIDEO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/thankyou.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}